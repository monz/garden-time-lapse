{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Time-Lapse Video at Noon\n",
    "\n",
    "Before we can create the time-lapse video, we must select images which should be present in the video. We already did this, using only one image for each day of the year, around 12am and 1pm, see [image-selection-noon](./1.0-image-selection-noon.ipynb) notebook.\n",
    "\n",
    "The result of the image selection process, was a list of images potentially selected for the time-lapse video. Why potentially? Because we do not know whether the selected images correspond to the *open* or *closed* class, yet. For this we use our already trained *logistic-regression* model to determine the class of each image. Once we classified each image, we can create our time-lapse video, considering only images related to the *open* class.\n",
    "\n",
    "We load our pre-trained model from the filesystem and predict the class for each image in the image directory given to the model. Features must not be extracted manually, this will automatically be done by the special prediction method call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'closed' images: 26 out of 363\n",
      "Paths of 'closed' files\n",
      "7      ../data/processed/selected-images-noon/pic_149...\n",
      "14     ../data/processed/selected-images-noon/pic_149...\n",
      "19     ../data/processed/selected-images-noon/pic_149...\n",
      "34     ../data/processed/selected-images-noon/pic_150...\n",
      "42     ../data/processed/selected-images-noon/pic_150...\n",
      "50     ../data/processed/selected-images-noon/pic_150...\n",
      "52     ../data/processed/selected-images-noon/pic_150...\n",
      "54     ../data/processed/selected-images-noon/pic_150...\n",
      "69     ../data/processed/selected-images-noon/pic_151...\n",
      "90     ../data/processed/selected-images-noon/pic_150...\n",
      "99     ../data/processed/selected-images-noon/pic_150...\n",
      "110    ../data/processed/selected-images-noon/pic_150...\n",
      "111    ../data/processed/selected-images-noon/pic_150...\n",
      "114    ../data/processed/selected-images-noon/pic_150...\n",
      "116    ../data/processed/selected-images-noon/pic_149...\n",
      "142    ../data/processed/selected-images-noon/pic_150...\n",
      "162    ../data/processed/selected-images-noon/pic_151...\n",
      "180    ../data/processed/selected-images-noon/pic_150...\n",
      "181    ../data/processed/selected-images-noon/pic_150...\n",
      "184    ../data/processed/selected-images-noon/pic_150...\n",
      "225    ../data/processed/selected-images-noon/pic_149...\n",
      "239    ../data/processed/selected-images-noon/pic_149...\n",
      "279    ../data/processed/selected-images-noon/pic_150...\n",
      "296    ../data/processed/selected-images-noon/pic_149...\n",
      "301    ../data/processed/selected-images-noon/pic_150...\n",
      "326    ../data/processed/selected-images-noon/pic_150...\n",
      "Name: file, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from src.models import predict_model\n",
    "from src.features import build_features\n",
    "\n",
    "IMG_DIR = '../data/processed/selected-images-noon'\n",
    "MODEL_PATH = \"../models/logreg-clf-notebook.joblib\"\n",
    "CLOSED_CLASS = 1\n",
    "\n",
    "# extract features from images\n",
    "data = build_features.get_exif_data(IMG_DIR, build_features.EXIF_TAGS)\n",
    "feature_columns = build_features.EXIF_TAGS.keys()\n",
    "\n",
    "# predict class using pre-trained logistic-regression model\n",
    "clf = predict_model.load_model(MODEL_PATH)\n",
    "prediction = predict_model.predict(data[feature_columns], clf)\n",
    "data['prediction'] = prediction\n",
    "\n",
    "# how many images were detected as 'closed'\n",
    "closed_total = data['prediction'].sum()\n",
    "print(\"Number of 'closed' images: {} out of {}\".format(closed_total, data.shape[0]))\n",
    "\n",
    "# display all rows where predicted class was 'closed'\n",
    "print(\"Paths of 'closed' files\")\n",
    "closed_file_paths = data[data['prediction'] == CLOSED_CLASS]['file']\n",
    "print(closed_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model determined **26** out of **363** images to be in the *closed* class. Some will notice that we only have selected 363 images, instead of having 365 to span an entire year. It is possible that there were no images for some days in the given timeframe, *noon +/- 5 minutes*, and therefore, not every day has been considered  in the selection process.\n",
    "\n",
    "For the time-lapse video, select images of class *open*, only. Therefore, we save the file paths identified to be member of the *open* class into a text file. This text file will later be used as list of selected images for video creation on the linux command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write image paths to file, which were identified to be member in *open* class\n",
    "OPEN_IMAGES_FILE = '../data/processed/file-paths-noon-open.txt'\n",
    "OPEN_CLASS = 0\n",
    "\n",
    "open_file_paths = data[data['prediction'] == OPEN_CLASS]['file']\n",
    "\n",
    "open_file_paths.to_csv(OPEN_IMAGES_FILE, header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Time-Lapse video\n",
    "\n",
    "Use *ffmpeg* to create the time-lapse video. The next command consists of multiple steps, *piped* or *chained* together to form a *single* command, which does the following\n",
    "\n",
    "1. read each file path from file we just created, using *cat*\n",
    "1. search for filenames, using regex pattern, *grep*\n",
    "1. sort filenames, to get them in ascending order\n",
    "1. use *xargs* to execute *find* for each filename\n",
    "1. use *find* to get file path, exec *cat* to get *bytes* of each file\n",
    "1. use *ffmpeg* to create time-lapse video, incl. timestamp as watermark, which is read from images' EXIF metadata\n",
    "\n",
    "\n",
    "```\n",
    "$ cat ../data/processed/file-paths-noon-open.txt | \\\n",
    "    grep -oE \"pic_[0-9]{10}.jpg\" | \\\n",
    "    sort | \\\n",
    "    xargs -I# find ../data/interim/selected-images-noon -name \"*#*\" -exec cat {} \\; | \\\n",
    "    ffmpeg -r 24 -f image2pipe -i - -filter_complex \"drawtext=fontfile=/Windows/Fonts/arial.ttf:text='timestamp \\: %{metadata\\:DateTime}':x=5:y=5:fontsize=24:fontcolor=white@0.9:box=1:boxcolor=blue@0.6\" -s 1024x768 -vcodec libx264 time-lapse-noon.mp4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "COMMAND = 'cat {filenames} | \\\n",
    "    grep -oE \"pic_[0-9]{{10}}.jpg\" | \\\n",
    "    sort | \\\n",
    "    xargs -I# find {image_dir} -name \"*#*\" -exec cat {{}} \\; | \\\n",
    "    ffmpeg -r 24 -f image2pipe -i - -filter_complex \\\n",
    "        \"drawtext=fontfile={font_file}:text=\\'timestamp \\: %{{metadata\\:DateTime}}\\':x=5:y=5:fontsize=24:fontcolor=white@0.9:box=1:boxcolor=blue@0.6\" \\\n",
    "        -s 1024x768 -vcodec libx264 {video_file_name}'\n",
    "\n",
    "IMAGE_DIR = '../data/processed/selected-images-noon/' # trailing slash required, due to symlinking\n",
    "VIDEO_FILE_NAME = '../data/processed/time-lapse-noon.mp4'\n",
    "FONT_FILE = '/usr/share/fonts/TTF/DejaVuSans.ttf'\n",
    "\n",
    "# delete existing video, otherwise command will fail\n",
    "if os.path.exists(VIDEO_FILE_NAME):\n",
    "  os.remove(VIDEO_FILE_NAME)\n",
    "\n",
    "# execute command\n",
    "os.system(COMMAND.format(\n",
    "    filenames = OPEN_IMAGES_FILE,\n",
    "    image_dir = IMAGE_DIR,\n",
    "    video_file_name = VIDEO_FILE_NAME,\n",
    "    font_file = FONT_FILE) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "With a frame rate set to *24* images per second, we get a video of 14 seconds. It's quiet nice to see the vegetation in all four seasons.\n",
    "\n",
    "If we take a close look, we can determine that in a single frame there was a *closed shutter* image. This means that our trained logistic-regression classifier was not able to detect this image correctly, and we have one *false negative* in this case. The rest of the time-lapse video looks ok-ish, at least considering correct image classification, only. However, there are some frames showing clearly visible reflections and we may choose another time frame to reduce reflections, e.g. choose some time in the early afternoon.\n",
    "\n",
    "> **Note:** Embedded video might not show correctly, use [external link](https://drive.google.com/file/d/1y6d_VMWg0dm1nIgvE1iljanSD3jEP345/preview) to watch the video, instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Simple video example -->\n",
    "<video src=\"../data/processed/time-lapse-noon.mp4\" autoplay width=800px loop=true type=\"video/mp4\" controls>\n",
    "  Sorry, your browser doesn't support embedded videos, \n",
    "  but don't worry, you can <a href=\"../data/processed/time-lapse-noon.mp4\">download it</a>\n",
    "  and watch it with your favorite video player!\n",
    "</video>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
